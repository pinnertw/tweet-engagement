{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7770d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.utils import label, author_data, sample_submission, train, test, features_author, fill\n",
    "from src.utils import cost, write_submission_global, test_model ,final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b01d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data[\"engagement_lr\"] = np.load(\"aid/all_author_train_lr.npy\")\n",
    "author_data[\"engagement_lgbm\"] = np.load(\"aid/all_author_train_lr.npy\")\n",
    "\n",
    "def data_deal(df):\n",
    "    # Add mean engagement on historical data\n",
    "    df = df.join(author_data.groupby(\"author\").agg(\n",
    "        {\"engagement\" : \"mean\",\n",
    "         \"engagement_lr\" : \"mean\",\n",
    "         \"engagement_lgbm\" : \"mean\"}\n",
    "    ).fillna(fill).rename(columns={\"engagement\" : \"engagement_\"}), \n",
    "                 on=\"author\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5ac9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all answers with different model.\n",
    "# With all author data\n",
    "train_set = data_deal(train)[[\"followers\", \"engagement_\", \"engagement_lr\", \"engagement_lgbm\"]]\n",
    "test_set = data_deal(test)[[\"followers\", \"engagement_\", \"engagement_lr\", \"engagement_lgbm\"]]\n",
    "\n",
    "# With single author data\n",
    "train_set[\"single_author_lr\"] = np.load(\"aid/single_author_train_lr.npy\")\n",
    "train_set[\"single_author_lgbm\"] = np.load(\"aid/single_author_train_lgbm.npy\")\n",
    "test_set[\"single_author_lr\"] = np.load(\"aid/single_author_test_lr.npy\")\n",
    "test_set[\"single_author_lgbm\"] = np.load(\"aid/single_author_test_lgbm.npy\")\n",
    "\n",
    "# With all train data\n",
    "train_set[\"all_train_lr\"] = np.load(\"aid/all_train_train_lr.npy\")\n",
    "train_set[\"all_train_lgbm\"] = np.load(\"aid/all_train_train_lgbm.npy\")\n",
    "test_set[\"all_train_lr\"] = np.load(\"aid/all_train_test_lr.npy\")\n",
    "test_set[\"all_train_lgbm\"] = np.load(\"aid/all_train_test_lgbm.npy\")\n",
    "\n",
    "# Fillna\n",
    "na_values = {\n",
    "    \"followers\" : 50,\n",
    "    \"engagement_\" : fill,\n",
    "    \"engagement_lr\" : fill,\n",
    "    \"engagement_lgbm\" : fill,\n",
    "    \"single_author_lr\" : fill,\n",
    "    \"single_author_lgbm\" : fill,\n",
    "}\n",
    "train_set = train_set.fillna(na_values)\n",
    "test_set = test_set.fillna(na_values)\n",
    "\n",
    "# Label set\n",
    "label_set = train[\"engagement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2b807",
   "metadata": {},
   "source": [
    "#### Get features and split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a151b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Seperation train/test\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(train_set, \n",
    "                                                   label_set, \n",
    "                                                   train_size=0.75,\n",
    "                                                   random_state=0,\n",
    "                                                  )\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "test_data = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00af532",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb0d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23456.502643222397 23189.91335007892\n",
      "CPU times: user 63.3 ms, sys: 2.69 ms, total: 66 ms\n",
      "Wall time: 23.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model, cost_train, cost_test = test_model(LinearRegression(), X_train, X_test, Y_train, Y_test)\n",
    "lr_model = final_model(LinearRegression(), train_set, label_set)\n",
    "write_submission_global(lr_model, cost_train, cost_test, test_data, output_name=\"final_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ced9d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15306.283082578559 15156.263724193206\n",
      "CPU times: user 1.1 s, sys: 6.74 ms, total: 1.11 s\n",
      "Wall time: 96.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMRegressor\n",
    "lgbm_model, cost_train, cost_test = test_model(LGBMRegressor(num_leaves=10, \n",
    "                                                             max_depth=5, \n",
    "                                                             learning_rate=0.07, \n",
    "                                                             objective=\"mae\",\n",
    "                                                             random_state=0), \n",
    "                                              X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d874cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = final_model(LGBMRegressor(num_leaves=10, \n",
    "                                       max_depth=5, \n",
    "                                       learning_rate=0.07, \n",
    "                                       objective=\"mae\",\n",
    "                                       random_state=0), \n",
    "                         train_set, label_set)\n",
    "write_submission_global(lgbm_model, cost_train, cost_test, test_data, output_name=\"final_lgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41483d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(8, 8), activation='relu', \n",
    "             solver='adam', alpha=0.0001, batch_size='auto', \n",
    "             learning_rate='constant', learning_rate_init=0.001, \n",
    "             power_t=0.5, max_iter=1000, shuffle=True, random_state=0, \n",
    "             tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "             nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, \n",
    "             beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69d2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scaler_label = StandardScaler()\n",
    "Y_train_ = scaler_label.fit_transform(Y_train.values.reshape(-1, 1)).reshape(-1)\n",
    "Y_test_ = scaler_label.transform(Y_test.values.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d1c6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(8, 8), max_iter=1000, random_state=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "mlp_model.fit(X_train, Y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a298326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19058.47759606769 19193.013359472483\n"
     ]
    }
   ],
   "source": [
    "def local_cost(x, y):\n",
    "    return np.abs(x - y).mean()\n",
    "\n",
    "cost_train, cost_test = local_cost(mlp_model.predict(X_train) * scaler_label.scale_ + scaler_label.mean_, \n",
    "           Y_train_* scaler_label.scale_ + scaler_label.mean_), local_cost(mlp_model.predict(X_test) * scaler_label.scale_ + scaler_label.mean_, \n",
    "           Y_test_* scaler_label.scale_ + scaler_label.mean_)\n",
    "print(cost_train, cost_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40bc8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = final_model(MLPRegressor(hidden_layer_sizes=(8, 8), activation='relu', \n",
    "                         solver='adam', alpha=0.0001, batch_size='auto', \n",
    "                         learning_rate='constant', learning_rate_init=0.001, \n",
    "                         power_t=0.5, max_iter=1000, shuffle=True, random_state=0, \n",
    "                         tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                         nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, \n",
    "                         beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000),\n",
    "                         train_set, label_set)\n",
    "\n",
    "sample_submission[\"engagement\"] = mlp_model.predict(test_data)*scaler_label.scale_ + scaler_label.mean_\n",
    "sample_submission[\"engagement\"] = sample_submission[\"engagement\"] * (sample_submission[\"engagement\"] >= 0)\n",
    "sample_submission.to_csv(\"output/final_mlp_{:.2f}_{:.2f}.csv\".format(cost_train, cost_test), index=False)\n",
    "#write_submission_global(lgbm_model, cost_train, cost_test, test_data, output_name=\"final_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e462e",
   "metadata": {},
   "source": [
    "#### Simply use the optimization method to get the best weight among these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "900d211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_opt = data_deal(train)[[\"engagement_\", \"engagement_lr\", \"engagement_lgbm\"]]\n",
    "test_set_opt = data_deal(test)[[\"engagement_\", \"engagement_lr\", \"engagement_lgbm\"]]\n",
    "\n",
    "# With single author data\n",
    "train_set_opt[\"single_author_lr\"] = np.load(\"aid/single_author_train_lr.npy\")\n",
    "train_set_opt[\"single_author_lgbm\"] = np.load(\"aid/single_author_train_lgbm.npy\")\n",
    "test_set_opt[\"single_author_lr\"] = np.load(\"aid/single_author_test_lr.npy\")\n",
    "test_set_opt[\"single_author_lgbm\"] = np.load(\"aid/single_author_test_lgbm.npy\")\n",
    "\n",
    "# With all train data\n",
    "train_set_opt[\"all_train_lr\"] = np.load(\"aid/all_train_train_lr.npy\")\n",
    "train_set_opt[\"all_train_lgbm\"] = np.load(\"aid/all_train_train_lgbm.npy\")\n",
    "test_set_opt[\"all_train_lr\"] = np.load(\"aid/all_train_test_lr.npy\")\n",
    "test_set_opt[\"all_train_lgbm\"] = np.load(\"aid/all_train_test_lgbm.npy\")\n",
    "\n",
    "train_set_opt[\"intercept\"] = fill\n",
    "test_set_opt[\"intercept\"] = fill\n",
    "\n",
    "# Result above\n",
    "train_set_opt[\"final_lr\"] = lr_model.predict(train_set)\n",
    "train_set_opt[\"final_lgbm\"] = lgbm_model.predict(train_set)\n",
    "train_set_opt[\"final_mlp\"] = mlp_model.predict(train_set)\n",
    "test_set_opt[\"final_lr\"] = lr_model.predict(test_set)\n",
    "test_set_opt[\"final_lgbm\"] = lgbm_model.predict(test_set)\n",
    "test_set_opt[\"final_mlp\"] = mlp_model.predict(test_set)\n",
    "\n",
    "# Fillna\n",
    "na_values = {\n",
    "    \"engagement_\" : fill,\n",
    "    \"engagement_lr\" : fill,\n",
    "    \"engagement_lgbm\" : fill,\n",
    "    \"single_author_lr\" : fill,\n",
    "    \"single_author_lgbm\" : fill,\n",
    "}\n",
    "train_set_opt = train_set_opt.fillna(na_values)\n",
    "test_set_opt = test_set_opt.fillna(na_values)\n",
    "\n",
    "# Label set\n",
    "label_set = train[\"engagement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c8d98f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.49914819e-10 4.30702930e-09 6.83068927e-09 9.99999993e-01\n",
      " 4.86674261e-15] 15228.5396429263\n",
      "[1.91511598e-08 0.00000000e+00 0.00000000e+00 8.48545713e-08\n",
      " 1.80877765e-08 0.00000000e+00 2.23366475e-08 8.89304834e-09\n",
      " 0.00000000e+00 9.85157830e-01 1.48423429e-02] 15226.03995497098\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "for nb in [5, 11]:\n",
    "    if nb == 5:\n",
    "        features = [i for i in train_set_opt.columns if \"lgbm\" in i] + [\"intercept\"]\n",
    "    elif nb == 11:\n",
    "        features = train_set_opt.columns\n",
    "\n",
    "    bnds = ((0, None),) * nb\n",
    "    cons = ({'type': 'eq', 'fun': lambda x:  x.sum()-1,},)\n",
    "    def fun(x):\n",
    "        return np.abs((train_set_opt[features].values * x).sum(axis=1) - label_set).mean()\n",
    "\n",
    "    x0 = np.ones(nb) / nb\n",
    "    res = minimize(fun, x0, bounds=bnds, constraints=cons)\n",
    "    print(res.x, res.fun)\n",
    "    sample_submission[\"engagement\"] = (test_set_opt[features].values * res.x).sum(axis=1)\n",
    "    sample_submission.to_csv(\"output/final_opt_{}.csv\".format(nb), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
